{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP3Vvwf0QtBm"
      },
      "source": [
        "# Load kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7GcwWbqQvCF"
      },
      "outputs": [],
      "source": [
        "# JAX는 기본적으로 멀티스레딩을 사용,\n",
        "# os.fork()는 멀티스레딩 코드와 호환되지 않기 때문에 데드락(deadlock)이 발생\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "mp.set_start_method('spawn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skyd77HQQwSU"
      },
      "outputs": [],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTzF-jBUQxU7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7_BfRrUUe1U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# import datasets\n",
        "# from datasets import load_dataset, load_metric, Dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# from accelerate import notebook_launcher, Accelerator, PartialState\n",
        "# from accelerate.utils import write_basic_config\n",
        "# from accelerate.inference import prepare_pippy\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    set_seed,\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoConfig\n",
        ")\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import multiprocessing as mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nicm8emmUfsk"
      },
      "outputs": [],
      "source": [
        "# params\n",
        "# model_name = \"/kaggle/input/phi/transformers/1/1\"\n",
        "model_name = \"/kaggle/input/flan-t5/pytorch/small/2\"\n",
        "model_path = \"model_checkpoint.pth\"\n",
        "seed = 42\n",
        "# lora_r = 2\n",
        "# quantize_bit = 16\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "eps = 1e-9\n",
        "l1_rate = 1e-10\n",
        "batch_size = 48\n",
        "test_batch_size = 1\n",
        "max_len = 64\n",
        "test_max_len = 64\n",
        "# n_sample = 0.5\n",
        "n_split = 2\n",
        "n_epoch = 2\n",
        "n_split_back = 2\n",
        "n_epoch_back = 1\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk_QohwJUjZ-"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Aqndq4Q8Y2"
      },
      "source": [
        "# Preprocessing for Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEn6okVMQ9q3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDOaF3fPQ-ug"
      },
      "outputs": [],
      "source": [
        "clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbylORr-Q_37"
      },
      "outputs": [],
      "source": [
        "clf_train.loc[:, \"prompt\"] = clf_train[\"prompt\"].apply(lambda x: json.loads(x)[0])\n",
        "clf_train.loc[:, \"response_a\"] = clf_train[\"response_a\"].apply(lambda x: json.loads(x)[0])\n",
        "clf_train.loc[:, \"response_b\"] = clf_train[\"response_b\"].apply(lambda x: json.loads(x)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdyCOHs7RBB_"
      },
      "outputs": [],
      "source": [
        "clf_train = clf_train.dropna()\n",
        "clf_train = clf_train.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3RarOBKRCQu"
      },
      "outputs": [],
      "source": [
        "# clf_train['new_text'] = [ \"### prompt: \"+clf_train['prompt'][x]+\" ### response_a: \"+clf_train['response_a'][x]+\" ### response_b: \"+clf_train['response_b'][x] for x in range(len(clf_train)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2evwf4xjRDsb"
      },
      "outputs": [],
      "source": [
        "clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRv9278ZRFRA"
      },
      "outputs": [],
      "source": [
        "clf_train = clf_train[['prompt','response_a','response_b','target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCI6O6j_RGte"
      },
      "outputs": [],
      "source": [
        "clf_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mh_C8ZuRIke"
      },
      "outputs": [],
      "source": [
        "def cl(x):\n",
        "  if x == [1,0,0]:\n",
        "    return 0\n",
        "  elif x == [0,1,0]:\n",
        "    return 1\n",
        "  else :\n",
        "    return 2\n",
        "\n",
        "clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOpJGKBMRJFi"
      },
      "outputs": [],
      "source": [
        "clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))\n",
        "clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))\n",
        "clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Co-xUXpRKF-"
      },
      "outputs": [],
      "source": [
        "clf_train['len'] = clf_train['p_len'] + clf_train['a_len']+ clf_train['b_len']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmHXLkoGRLMK"
      },
      "outputs": [],
      "source": [
        "sample_df = clf_train.sample(int(len(clf_train)*n_sample), weights = \"len\", random_state=seed).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92xExNs_RMNQ"
      },
      "outputs": [],
      "source": [
        "sample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiKuShXfRNPN"
      },
      "outputs": [],
      "source": [
        "t_dat, v_dat = train_test_split(sample_df, test_size=0.2, random_state=42, stratify = sample_df['labels'])\n",
        "\n",
        "t_dat = t_dat.reset_index(drop=True)\n",
        "v_dat = v_dat.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II7GIxPVROb4"
      },
      "outputs": [],
      "source": [
        "t_dat = t_dat.drop( labels= 'target' , axis = 1)\n",
        "v_dat = v_dat.drop( labels= 'target' , axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY4fBa-zRPom"
      },
      "outputs": [],
      "source": [
        "t_dat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNJR3e5sRQ64"
      },
      "outputs": [],
      "source": [
        "np.unique(t_dat['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReOw92O4RSPs"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prompt = df['prompt']\n",
        "        self.response_a = df['response_a']\n",
        "        self.response_b = df['response_b']\n",
        "        self.max_len = max_len\n",
        "        self.targets = df.get('labels', None)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompt)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        prompt = str(self.prompt[index])\n",
        "        response_a = str(self.response_a[index])\n",
        "        response_b = str(self.response_b[index])\n",
        "\n",
        "        prompt_len = len(self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True)['input_ids'])\n",
        "        response_a_len = len(self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True)['input_ids'])\n",
        "        response_b_len = len(self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True)['input_ids'])\n",
        "\n",
        "        final_prompt_len = min(self.max_len, prompt_len)\n",
        "        final_a_len = min(self.max_len, response_a_len)\n",
        "        final_b_len = min(self.max_len, response_b_len)\n",
        "\n",
        "        prompt_token = self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
        "        response_a_token = self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
        "        response_b_token = self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
        "\n",
        "        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)\n",
        "        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)\n",
        "\n",
        "        if self.targets is not None:\n",
        "            labels = torch.LongTensor([self.targets[index]])\n",
        "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}\n",
        "        else:\n",
        "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMXsSEReRUB4"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch, tokenizer):\n",
        "\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None\n",
        "\n",
        "    # Find the maximum length of the sequences in the batch\n",
        "    max_len = max([input_id.size(0) for input_id in input_ids])\n",
        "\n",
        "    # Re-tokenize with the new max length\n",
        "    new_input_ids = []\n",
        "    new_attention_masks = []\n",
        "\n",
        "    for item in batch:\n",
        "        input_ids = item['input_ids'][:max_len]\n",
        "        attention_mask = item['attention_mask'][:max_len]\n",
        "\n",
        "        new_input_ids.append(input_ids)\n",
        "        new_attention_masks.append(attention_mask)\n",
        "\n",
        "    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    output = {\n",
        "    'input_ids': new_input_ids,\n",
        "    'attention_mask': new_attention_masks}\n",
        "\n",
        "    if labels is not None:\n",
        "        output['labels'] = labels\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVks06l9RVzc"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):\n",
        "    dataloader = DataLoader(\n",
        "        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_t8dlxxRXlj"
      },
      "outputs": [],
      "source": [
        "# tokenizer.decode([2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEjtnujRRYDQ"
      },
      "source": [
        "# Training Classifier Model\n",
        "\n",
        "offline : Unable to install libraries and learning time limit\n",
        "\n",
        "- Model: Phi\n",
        "- Quantization : FP32bit\n",
        " - Inapplicable\n",
        "- Adapter: add adaptor to query value r = 2\n",
        " - Create an attention layer to add adepter to the attention value and replace it with the existing model\n",
        " - Maintain existing weights\n",
        "- Add a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbU13vrARZPT"
      },
      "outputs": [],
      "source": [
        "# # # 모델 구성 가져오기\n",
        "# # config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "# # # 활성화 함수 설정 (예: gelu)\n",
        "# # config.hidden_activation = \"gelu\"\n",
        "# # config.use_cache = False\n",
        "\n",
        "# base_model = AutoModel.from_pretrained(model_name) #   ,  config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYt0CiZ0RbA4"
      },
      "outputs": [],
      "source": [
        "# base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6gUtmt-RcQC"
      },
      "outputs": [],
      "source": [
        "def quantize_tensor(tensor, num_bits=quantize_bit):\n",
        "    qmin = 0.\n",
        "    qmax = 2.**num_bits - 1.\n",
        "\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (qmax - qmin)\n",
        "    zero_point = qmin - min_val / scale\n",
        "\n",
        "    quantized_tensor = torch.round(tensor / scale + zero_point)\n",
        "    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)\n",
        "    quantized_tensor = (quantized_tensor - zero_point) * scale\n",
        "\n",
        "    return quantized_tensor\n",
        "\n",
        "def quantize_model(model, num_bits=8):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
        "        elif isinstance(module, nn.Conv2d):\n",
        "            module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# import torch.quantization\n",
        "\n",
        "# def quantize_model_dynamic(model):\n",
        "#     model.qconfig = torch.quantization.default_dynamic_qconfig\n",
        "#     torch.quantization.prepare(model, inplace=True)\n",
        "#     torch.quantization.convert(model, inplace=True)\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT6cyuzsRc1_"
      },
      "outputs": [],
      "source": [
        "class LoRA(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0):\n",
        "        super(LoRA, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.rank = rank\n",
        "        self.lora_a = nn.Linear(in_features, rank, bias=False)\n",
        "        self.lora_b = nn.Linear(rank, out_features, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.alpha * self.lora_b(self.lora_a(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OaVjlYdReAC"
      },
      "outputs": [],
      "source": [
        "class PhiRotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.base = base\n",
        "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "\n",
        "        # Build here to make `torch.jit.trace` work.\n",
        "        self._set_cos_sin_cache(\n",
        "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
        "        )\n",
        "\n",
        "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
        "        self.max_seq_len_cached = seq_len\n",
        "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
        "\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
        "        if seq_len > self.max_seq_len_cached:\n",
        "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
        "\n",
        "        return (\n",
        "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
        "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
        "        )\n",
        "\n",
        "\n",
        "class PhiLinearScalingRotaryEmbedding(PhiRotaryEmbedding):\n",
        "    \"\"\"PhiRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n",
        "\n",
        "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
        "        self.scaling_factor = scaling_factor\n",
        "        super().__init__(dim, max_position_embeddings, base, device)\n",
        "\n",
        "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
        "        self.max_seq_len_cached = seq_len\n",
        "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
        "        t = t / self.scaling_factor\n",
        "\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.falcon.modeling_falcon.FalconDynamicNTKScalingRotaryEmbedding with Falcon->Phi\n",
        "class PhiDynamicNTKScalingRotaryEmbedding(PhiRotaryEmbedding):\n",
        "    \"\"\"PhiRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n",
        "\n",
        "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
        "        self.scaling_factor = scaling_factor\n",
        "        super().__init__(dim, max_position_embeddings, base, device)\n",
        "\n",
        "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
        "        self.max_seq_len_cached = seq_len\n",
        "\n",
        "        if seq_len > self.max_position_embeddings:\n",
        "            base = self.base * (\n",
        "                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n",
        "            ) ** (self.dim / (self.dim - 2))\n",
        "            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
        "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "\n",
        "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
        "\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
        "\n",
        "\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTWltTdSRgB3"
      },
      "outputs": [],
      "source": [
        "class PhiAttention(nn.Module):\n",
        "    def __init__(self, config,layer_idx, rank=lora_r):\n",
        "        super(PhiAttention, self).__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "        self.attention_dropout = config.attention_dropout\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "        self.rope_theta = config.rope_theta\n",
        "        self.partial_rotary_factor = config.partial_rotary_factor\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(\n",
        "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
        "                f\" and `num_heads`: {self.num_heads}).\"\n",
        "            )\n",
        "\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n",
        "        self.dense = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=True)\n",
        "\n",
        "\n",
        "        # LoRA adapters for query and value\n",
        "        # self.lora_q = LoRA(config.hidden_size, self.num_heads * self.head_dim, rank)\n",
        "        self.lora_v = LoRA(self.hidden_size, self.num_key_value_heads * self.head_dim, rank)\n",
        "\n",
        "\n",
        "        self.qk_layernorm = config.qk_layernorm\n",
        "        if self.qk_layernorm:\n",
        "            self.q_layernorm = nn.LayerNorm(\n",
        "                config.hidden_size // self.num_heads, eps=config.layer_norm_eps, elementwise_affine=True\n",
        "            )\n",
        "            self.k_layernorm = nn.LayerNorm(\n",
        "                config.hidden_size // self.num_heads, eps=config.layer_norm_eps, elementwise_affine=True\n",
        "            )\n",
        "\n",
        "        self._init_rope()\n",
        "\n",
        "    def _init_rope(self):\n",
        "        if self.config.rope_scaling is None:\n",
        "            self.rotary_emb = PhiRotaryEmbedding(\n",
        "                int(self.partial_rotary_factor * self.head_dim),\n",
        "                max_position_embeddings=self.max_position_embeddings,\n",
        "                base=self.rope_theta,\n",
        "            )\n",
        "        else:\n",
        "            scaling_type = self.config.rope_scaling[\"type\"]\n",
        "            scaling_factor = self.config.rope_scaling[\"factor\"]\n",
        "            if scaling_type == \"linear\":\n",
        "                self.rotary_emb = PhiLinearScalingRotaryEmbedding(\n",
        "                    int(self.partial_rotary_factor * self.head_dim),\n",
        "                    max_position_embeddings=self.max_position_embeddings,\n",
        "                    scaling_factor=scaling_factor,\n",
        "                    base=self.rope_theta,\n",
        "                )\n",
        "            elif scaling_type == \"dynamic\":\n",
        "                self.rotary_emb = PhiDynamicNTKScalingRotaryEmbedding(\n",
        "                    int(self.partial_rotary_factor * self.head_dim),\n",
        "                    max_position_embeddings=self.max_position_embeddings,\n",
        "                    scaling_factor=scaling_factor,\n",
        "                    base=self.rope_theta,\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask = None,\n",
        "        position_ids = None,\n",
        "        past_key_value = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False\n",
        "    ):\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        query_states = self.q_proj(hidden_states) #+ self.lora_q(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states) + self.lora_v(hidden_states)\n",
        "\n",
        "        if self.qk_layernorm:\n",
        "            query_states = self.q_layernorm(query_states)\n",
        "            key_states = self.k_layernorm(key_states)\n",
        "\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        kv_seq_len = key_states.shape[-2]\n",
        "        if past_key_value is not None:\n",
        "            if self.layer_idx is None:\n",
        "                raise ValueError(\n",
        "                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n",
        "                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n",
        "                    \"with a layer index.\"\n",
        "                )\n",
        "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
        "\n",
        "        # Partial rotary embedding\n",
        "        query_rot, query_pass = (\n",
        "            query_states[..., : self.rotary_emb.dim],\n",
        "            query_states[..., self.rotary_emb.dim :],\n",
        "        )\n",
        "        key_rot, key_pass = (\n",
        "            key_states[..., : self.rotary_emb.dim],\n",
        "            key_states[..., self.rotary_emb.dim :],\n",
        "        )\n",
        "        # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n",
        "\n",
        "        cos = cos[position_ids].unsqueeze(1)\n",
        "        sin = sin[position_ids].unsqueeze(1)\n",
        "        query_rot = (query_rot * cos) + (rotate_half(query_rot) * sin)\n",
        "        key_rot = (key_rot * cos) + (rotate_half(key_rot) * sin)\n",
        "\n",
        "\n",
        "        # [batch_size, seq_length, num_heads, head_dim]\n",
        "        query_states = torch.cat((query_rot, query_pass), dim=-1)\n",
        "        key_states = torch.cat((key_rot, key_pass), dim=-1)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"partial_rotation_size\": self.rotary_emb.dim}\n",
        "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Queries and keys upcast to fp32 is required by Phi-2 to avoid overflow\n",
        "        attn_weights = torch.matmul(\n",
        "            query_states.to(torch.float32), key_states.to(torch.float32).transpose(2, 3)\n",
        "        ) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        # upcast attention to fp32\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n",
        "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        attn_output = self.dense(attn_output)\n",
        "\n",
        "        if not output_attentions:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlDLK3nnRiaX"
      },
      "outputs": [],
      "source": [
        "def replace_attention_module(config,layer,layer_idx):\n",
        "    if hasattr(layer, 'self_attn') and layer_idx//2 == 0:\n",
        "\n",
        "        new_attention = PhiAttention(config,layer_idx)\n",
        "\n",
        "        # 쿼리, 키, 값 프로젝션 가중치 및 바이어스를 복사\n",
        "        new_attention.q_proj.weight.data.copy_(layer.self_attn.q_proj.weight.data)\n",
        "        new_attention.k_proj.weight.data.copy_(layer.self_attn.k_proj.weight.data)\n",
        "        new_attention.v_proj.weight.data.copy_(layer.self_attn.v_proj.weight.data)\n",
        "        new_attention.dense.weight.data.copy_(layer.self_attn.dense.weight.data)\n",
        "\n",
        "        # 기존 레이어의 self_attn 모듈을 새로운 모듈로 교체\n",
        "        layer.self_attn = new_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ0ovsnERjjc"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "class LoraModelForClassification(nn.Module):\n",
        "    def __init__(self, lora_model):  # config 추가\n",
        "        super(LoraModelForClassification, self).__init__()\n",
        "        self.config = lora_model.config  # config 설정\n",
        "        self.peft_model = lora_model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, 3)\n",
        "#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)\n",
        "#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.peft_model(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state.mean(dim =1)\n",
        "        output_dropout = self.dropout(pooled_output)\n",
        "        logits = self.classifier(output_dropout)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          labels = labels\n",
        "          loss = loss_fn(logits, labels)\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38V-FjvRlLt"
      },
      "source": [
        "## Parallel Training\n",
        "\n",
        "hugging face accelarate 학습 코드 이용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeSSmcEoRmQV"
      },
      "outputs": [],
      "source": [
        "# from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# AMP를 위한 GradScaler 설정 float32+float16 연산\n",
        "# scaler = GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnx459qRRnX1"
      },
      "outputs": [],
      "source": [
        "def parallel_function(model_name):\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    model = AutoModel.from_pretrained(model_name,torch_dtype=torch.float16) #   ,  config=config)\n",
        "    model = quantize_model(model)\n",
        "    for idx, layer in enumerate(model.layers):\n",
        "        replace_attention_module(model.config,layer,idx)\n",
        "    model = LoraModelForClassification(model)\n",
        "\n",
        "    for param in model.peft_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, PhiAttention):\n",
        "            # lora_A 및 lora_B의 학습 가능 여부를 설정합니다.\n",
        "    #         module.lora_q.lora_a.weight.requires_grad = True\n",
        "    #         module.lora_q.lora_b.weight.requires_grad = True\n",
        "            module.lora_v.lora_a.weight.requires_grad = True\n",
        "            module.lora_v.lora_b.weight.requires_grad = True\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
        "    print(f\"Total trainable parameters: {total_params}\")\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_dataloader = create_dataloaders(t_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)\n",
        "\n",
        "    eval_dataloader = create_dataloaders(v_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)\n",
        "\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=100,\n",
        "        num_training_steps=len(train_dataloader) * num_epochs,\n",
        "    )\n",
        "\n",
        "    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            loss, _ = model(**batch)\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            with torch.no_grad():\n",
        "                _, logits = model(**batch)\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            all_predictions.append(accelerator.gather(predictions))\n",
        "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
        "\n",
        "        all_predictions = torch.cat(all_predictions)[:len(eval_dataloader)].cpu()\n",
        "        all_labels = torch.cat(all_labels)[:len(eval_dataloader)].cpu()\n",
        "\n",
        "        acc_metric = accuracy_score(all_labels, all_predictions)\n",
        "        eval_metric = f1_score(all_labels, all_predictions, average=\"macro\")\n",
        "\n",
        "        accelerator.print(f\"epoch: {epoch} \\n accuracy: {acc_metric:.3f} \\n f1 score: {eval_metric:.3f}\")\n",
        "\n",
        "\n",
        "    model = accelerator.unwrap_model(model)\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # 모델 상태 사전 저장\n",
        "    if accelerator.is_main_process:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # 동기화 완료 메시지\n",
        "    accelerator.wait_for_everyone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goUL5ZRJRpHH"
      },
      "outputs": [],
      "source": [
        "notebook_launcher(parallel_function, args=(model_name,), num_processes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxUmj2BERrhf"
      },
      "outputs": [],
      "source": [
        "# config_path = '/root/.cache/huggingface/accelerate/default_config.yaml'\n",
        "# if os.path.exists(config_path):\n",
        "#     os.remove(config_path)\n",
        "#     print(f\"{config_path} has been removed.\")\n",
        "\n",
        "# 새로운 설정 파일 작성\n",
        "# write_basic_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaRYAHm6RshH"
      },
      "outputs": [],
      "source": [
        "# !cat /root/.cache/huggingface/accelerate/default_config.yaml"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
