{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP3Vvwf0QtBm"
      },
      "source": [
        "# Load kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7GcwWbqQvCF"
      },
      "outputs": [],
      "source": [
        "# JAX는 기본적으로 멀티스레딩을 사용,\n",
        "# os.fork()는 멀티스레딩 코드와 호환되지 않기 때문에 데드락(deadlock)이 발생\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "mp.set_start_method('spawn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skyd77HQQwSU"
      },
      "outputs": [],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTzF-jBUQxU7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Aqndq4Q8Y2"
      },
      "source": [
        "# Preprocessing for Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEn6okVMQ9q3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDOaF3fPQ-ug"
      },
      "outputs": [],
      "source": [
        "clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbylORr-Q_37"
      },
      "outputs": [],
      "source": [
        "clf_train.loc[:, \"prompt\"] = clf_train[\"prompt\"].apply(lambda x: json.loads(x)[0])\n",
        "clf_train.loc[:, \"response_a\"] = clf_train[\"response_a\"].apply(lambda x: json.loads(x)[0])\n",
        "clf_train.loc[:, \"response_b\"] = clf_train[\"response_b\"].apply(lambda x: json.loads(x)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdyCOHs7RBB_"
      },
      "outputs": [],
      "source": [
        "clf_train = clf_train.dropna()\n",
        "clf_train = clf_train.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3RarOBKRCQu"
      },
      "outputs": [],
      "source": [
        "# clf_train['new_text'] = [ \"### prompt: \"+clf_train['prompt'][x]+\" ### response_a: \"+clf_train['response_a'][x]+\" ### response_b: \"+clf_train['response_b'][x] for x in range(len(clf_train)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2evwf4xjRDsb"
      },
      "outputs": [],
      "source": [
        "clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRv9278ZRFRA"
      },
      "outputs": [],
      "source": [
        "clf_train = clf_train[['prompt','response_a','response_b','target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCI6O6j_RGte"
      },
      "outputs": [],
      "source": [
        "clf_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mh_C8ZuRIke"
      },
      "outputs": [],
      "source": [
        "def cl(x):\n",
        "  if x == [1,0,0]:\n",
        "    return 0\n",
        "  elif x == [0,1,0]:\n",
        "    return 1\n",
        "  else :\n",
        "    return 2\n",
        "\n",
        "clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOpJGKBMRJFi"
      },
      "outputs": [],
      "source": [
        "clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))\n",
        "clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))\n",
        "clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Co-xUXpRKF-"
      },
      "outputs": [],
      "source": [
        "clf_train['len'] = clf_train['p_len'] + clf_train['a_len']+ clf_train['b_len']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmHXLkoGRLMK"
      },
      "outputs": [],
      "source": [
        "sample_df = clf_train.sample(int(len(clf_train)*n_sample), weights = \"len\", random_state=seed).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92xExNs_RMNQ"
      },
      "outputs": [],
      "source": [
        "sample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiKuShXfRNPN"
      },
      "outputs": [],
      "source": [
        "t_dat, v_dat = train_test_split(sample_df, test_size=0.2, random_state=42, stratify = sample_df['labels'])\n",
        "\n",
        "t_dat = t_dat.reset_index(drop=True)\n",
        "v_dat = v_dat.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II7GIxPVROb4"
      },
      "outputs": [],
      "source": [
        "t_dat = t_dat.drop( labels= 'target' , axis = 1)\n",
        "v_dat = v_dat.drop( labels= 'target' , axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY4fBa-zRPom"
      },
      "outputs": [],
      "source": [
        "t_dat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNJR3e5sRQ64"
      },
      "outputs": [],
      "source": [
        "np.unique(t_dat['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReOw92O4RSPs"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prompt = df['prompt']\n",
        "        self.response_a = df['response_a']\n",
        "        self.response_b = df['response_b']\n",
        "        self.max_len = max_len\n",
        "        self.targets = df.get('labels', None)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompt)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        prompt = str(self.prompt[index])\n",
        "        response_a = str(self.response_a[index])\n",
        "        response_b = str(self.response_b[index])\n",
        "\n",
        "        prompt_len = len(self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True)['input_ids'])\n",
        "        response_a_len = len(self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True)['input_ids'])\n",
        "        response_b_len = len(self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True)['input_ids'])\n",
        "\n",
        "        final_prompt_len = min(self.max_len, prompt_len)\n",
        "        final_a_len = min(self.max_len, response_a_len)\n",
        "        final_b_len = min(self.max_len, response_b_len)\n",
        "\n",
        "        prompt_token = self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
        "        response_a_token = self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
        "        response_b_token = self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
        "\n",
        "        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)\n",
        "        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)\n",
        "\n",
        "        if self.targets is not None:\n",
        "            labels = torch.LongTensor([self.targets[index]])\n",
        "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}\n",
        "        else:\n",
        "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMXsSEReRUB4"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch, tokenizer):\n",
        "\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None\n",
        "\n",
        "    # Find the maximum length of the sequences in the batch\n",
        "    max_len = max([input_id.size(0) for input_id in input_ids])\n",
        "\n",
        "    # Re-tokenize with the new max length\n",
        "    new_input_ids = []\n",
        "    new_attention_masks = []\n",
        "\n",
        "    for item in batch:\n",
        "        input_ids = item['input_ids'][:max_len]\n",
        "        attention_mask = item['attention_mask'][:max_len]\n",
        "\n",
        "        new_input_ids.append(input_ids)\n",
        "        new_attention_masks.append(attention_mask)\n",
        "\n",
        "    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    output = {\n",
        "    'input_ids': new_input_ids,\n",
        "    'attention_mask': new_attention_masks}\n",
        "\n",
        "    if labels is not None:\n",
        "        output['labels'] = labels\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVks06l9RVzc"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):\n",
        "    dataloader = DataLoader(\n",
        "        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_t8dlxxRXlj"
      },
      "outputs": [],
      "source": [
        "# tokenizer.decode([2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2UPWtR4Rt8u"
      },
      "source": [
        "# Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94muGvBKRvgF"
      },
      "outputs": [],
      "source": [
        "# model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "# model = quantize_model(model)\n",
        "# for idx, layer in enumerate(model.layers):\n",
        "#     replace_attention_module(model.config,layer,idx)\n",
        "# model = LoraModelForClassification(model)\n",
        "# model.load_state_dict(torch.load(model_path))\n",
        "# device = \"cuda:0\"\n",
        "# model.to(device)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR2M-ZavRwnJ"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
        "len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM1tpvnwRxqL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "test[\"prompt\"] = test[\"prompt\"].apply(lambda x: json.loads(x)[0])\n",
        "test[\"response_a\"] = test[\"response_a\"].apply(lambda x: json.loads(x)[0])\n",
        "test[\"response_b\"] = test[\"response_b\"].apply(lambda x: json.loads(x)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbFMSbW0RzBj"
      },
      "outputs": [],
      "source": [
        "# test['new_text'] = [ \"### prompt: \"+test['prompt'][x]+\" ### response_a: \"+test['response_a'][x]+\" ### response_b: \"+test['response_b'][x] for x in range(len(test)) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB1OWM04R0JJ"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nAB_7mUR1Fp"
      },
      "outputs": [],
      "source": [
        "test_0 = test[:len(test)//2].reset_index(drop=True)\n",
        "test_1 = test[len(test)//2:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ0L-RgiR2IL"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast\n",
        "\n",
        "def infer(model, dataloader, device):\n",
        "#     model = nn.DataParallel(model)  # Wrap the model with DataParallel\n",
        "#     model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    target_list = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        with torch.no_grad():\n",
        "            with autocast():\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                _,logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                softmax_logits = torch.nn.functional.softmax(logits, dim=1)\n",
        "                target_list.append(softmax_logits)\n",
        "\n",
        "    return target_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XXQAuarR3hq"
      },
      "outputs": [],
      "source": [
        "from threading import Thread\n",
        "\n",
        "gpu0 = \"cuda:0\"\n",
        "gpu1 = \"cuda:1\"\n",
        "\n",
        "model0 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "model0 = quantize_model(model0)\n",
        "for idx, layer in enumerate(model0.layers):\n",
        "    replace_attention_module(model0.config,layer,idx)\n",
        "model0 = LoraModelForClassification(model0)\n",
        "model0.load_state_dict(torch.load(model_path))\n",
        "model0 = model0.to(gpu0)\n",
        "\n",
        "model1 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "model1 = quantize_model(model1)\n",
        "for idx, layer in enumerate(model1.layers):\n",
        "    replace_attention_module(model1.config,layer,idx)\n",
        "model1 = LoraModelForClassification(model1)\n",
        "model1.load_state_dict(torch.load(model_path))\n",
        "model1 = model1.to(gpu1)\n",
        "\n",
        "\n",
        "tokenizer0 = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer0.pad_token is None:\n",
        "    tokenizer0.pad_token = tokenizer0.eos_token\n",
        "tokenizer0.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
        "\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer1.pad_token is None:\n",
        "    tokenizer1.pad_token = tokenizer1.eos_token\n",
        "tokenizer1.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
        "\n",
        "test_dataloader0 = create_dataloaders(test_0,tokenizer0,test_max_len,test_batch_size, shuffle = False)\n",
        "test_dataloader1 = create_dataloaders(test_1,tokenizer1,test_max_len,test_batch_size, shuffle = False)\n",
        "\n",
        "def run_inference(model, dataloader, device, results, index):\n",
        "    results[index] = infer(model, dataloader, device)\n",
        "\n",
        "results = {}\n",
        "\n",
        "process0 = Thread(target=run_inference, args=(model0, test_dataloader0, gpu0, results,0))\n",
        "process1 = Thread(target=run_inference, args=(model1, test_dataloader1, gpu1, results,1))\n",
        "\n",
        "# Start the processes\n",
        "process0.start()\n",
        "process1.start()\n",
        "\n",
        "# Wait for both processes to finish\n",
        "process0.join()\n",
        "process1.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z-xVTQ_R5KZ"
      },
      "outputs": [],
      "source": [
        "# target_list = []\n",
        "\n",
        "# for data in valid_dataloader:\n",
        "#   with torch.no_grad():\n",
        "#     input_ids = data['input_ids'].to(device = device, dtype = torch.long)\n",
        "#     attention_mask = data['attention_mask'].to(device = device, dtype = torch.long)\n",
        "#     _, logits = model(input_ids, attention_mask)\n",
        "\n",
        "# target_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYbxvANFR6Uf"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0'  # 이동할 장치 선택\n",
        "for k, v in results.items():\n",
        "    for i in range(len(v)):\n",
        "        results[k][i] = v[i].to(device)\n",
        "\n",
        "# 딕셔너리의 값을 하나로 합치기\n",
        "target_list = torch.cat([torch.cat(v, dim=0) for v in results.values()], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYSS0ivnR7n7"
      },
      "outputs": [],
      "source": [
        "sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
        "# sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWJD-LbTR8oJ"
      },
      "outputs": [],
      "source": [
        "df_list = []\n",
        "for tensor in target_list:\n",
        "    df = pd.DataFrame(tensor.unsqueeze(0).detach().cpu().numpy(), columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
        "    df_list.append(df)\n",
        "\n",
        "combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "sub = sub.set_index(pd.Index(combined_df.index))\n",
        "\n",
        "final_df = pd.concat([sub[['id']], combined_df], axis=1)\n",
        "# final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-NqBYNzR-BZ"
      },
      "outputs": [],
      "source": [
        "def delete_files_and_folders(path):\n",
        "    # 경로가 존재하는지 확인\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Error: {path} does not exist.\")\n",
        "        return\n",
        "\n",
        "    # 경로 내의 모든 파일 및 폴더를 탐색\n",
        "    for root, dirs, files in os.walk(path, topdown=False):\n",
        "        # 파일 삭제\n",
        "        for name in files:\n",
        "            if name == \"submission.csv\":\n",
        "                print(f\"Skipping file: {os.path.join(root, name)}\")\n",
        "                continue\n",
        "            file_path = os.path.join(root, name)\n",
        "            print(f\"Deleting file: {file_path}\")\n",
        "            os.remove(file_path)\n",
        "\n",
        "#         # 폴더 삭제\n",
        "#         for name in dirs:\n",
        "#             folder_path = os.path.join(root, name)\n",
        "#             print(f\"Deleting folder: {folder_path}\")\n",
        "#             shutil.rmtree(folder_path)\n",
        "\n",
        "    print(f\"All files and folders in {path} have been deleted.\")\n",
        "\n",
        "# 예제 경로\n",
        "path_to_delete = \"/kaggle/working/\"\n",
        "\n",
        "# 파일 및 폴더 삭제 함수 호출\n",
        "delete_files_and_folders(path_to_delete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTKoLTjIR_jB"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWlSsWYoSA0-"
      },
      "outputs": [],
      "source": [
        "# GPU 메모리 비우기\n",
        "def clear_gpu_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# 학습 후 GPU 메모리 비우기\n",
        "clear_gpu_memory()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
